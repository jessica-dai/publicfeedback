---
layout: home
title: Public Feedback for AI Evals
---

AI evals are missing two key ingredients: First, substantive analysis of the post-deployment phase, as real-world users interact with AI systems for their actual use cases; second, the ability for end-users, or members of the general public, to provide feedback that reflects their experiences. After all, it is not only AI researchers and developers who can form judgments on ``good'' or ``bad'' system behavior. And yet: in other domains, collecting and analyzing ``reports'' is a well-established protocol for ensuring ongoing safety of high-stakes systems in real time. 

What would it take to get this to work for AI systems? This website highlights research papers and product prototypes that seek to make this vision a reality. 
As a starting point, we recommend the following collection of manuscripts that sketch out relevant legal, technical, and policy frameworks; while their proposals are simular, each provides complementary analysis. 

* **Sample forms and policy recommendations** —  
  [In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI](https://arxiv.org/abs/2503.16861){:target="_blank"} (Longpre et al., 2025)

* **Substantive and normative arguments and future research directions** —  
  [Aggregated Individual Reporting for Post-Deployment Evaluation](https://arxiv.org/abs/2506.18133){:target="_blank"} (Dai et al., 2025)

* **Design principles from reporting systems in other domains** —  
  [Known Unknowns and Unknown Unknowns: Designing a Scalable Adverse Event Reporting System for AI](https://ojs.aaai.org/index.php/AIES/article/view/36607){:target="_blank"} (Gailmard et al., 2025)


Together, these works give one pathway towards concrete mechanisms for safety and accountability in deployed AI systems.

<!-- 
#### Research

#### Real-world deployments 

#### Disambiguation
* Why not ai incidents dot com (or OECD etc)?
* 
-->
